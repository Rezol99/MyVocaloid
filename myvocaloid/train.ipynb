{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b7418fb-a110-420c-b1c6-6bbaf86b4787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lyric_indexs.shape (50872, 1)\n",
      "durations.shape (50872, 1, 1)\n",
      "notenums.shahpe (50872, 1, 1)\n",
      "y.shape (50872, 128, 5)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735904729.085632  488864 cuda_dnn.cc:529] Loaded cuDNN version 90600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 21ms/step - loss: 0.0807 - mae: 0.1042 - val_loss: 8.3822e-04 - val_mae: 0.0041\n",
      "Epoch 2/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 8.0653e-04 - mae: 0.0041 - val_loss: 8.4096e-04 - val_mae: 0.0042\n",
      "Epoch 3/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 8.4171e-04 - mae: 0.0038 - val_loss: 8.3000e-04 - val_mae: 0.0029\n",
      "Epoch 4/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - loss: 8.2647e-04 - mae: 0.0039 - val_loss: 8.0264e-04 - val_mae: 0.0040\n",
      "Epoch 5/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 5.5650e-04 - mae: 0.0050 - val_loss: 2.6163e-04 - val_mae: 0.0032\n",
      "Epoch 6/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 2.9943e-04 - mae: 0.0035 - val_loss: 2.6364e-04 - val_mae: 0.0033\n",
      "Epoch 7/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - loss: 2.4886e-04 - mae: 0.0031 - val_loss: 1.4491e-04 - val_mae: 0.0020\n",
      "Epoch 8/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - loss: 2.2706e-04 - mae: 0.0026 - val_loss: 1.6391e-04 - val_mae: 0.0030\n",
      "Epoch 9/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - loss: 2.0696e-04 - mae: 0.0027 - val_loss: 2.2775e-04 - val_mae: 0.0054\n",
      "Epoch 10/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - loss: 1.9079e-04 - mae: 0.0025 - val_loss: 1.5212e-04 - val_mae: 0.0031\n",
      "Epoch 11/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 2.0548e-04 - mae: 0.0027 - val_loss: 1.2706e-04 - val_mae: 0.0013\n",
      "Epoch 12/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 1.7802e-04 - mae: 0.0023 - val_loss: 1.2089e-04 - val_mae: 0.0033\n",
      "Epoch 13/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 2.1000e-04 - mae: 0.0025 - val_loss: 1.3357e-04 - val_mae: 0.0013\n",
      "Epoch 14/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - loss: 1.7746e-04 - mae: 0.0024 - val_loss: 9.8336e-05 - val_mae: 0.0015\n",
      "Epoch 15/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - loss: 1.7871e-04 - mae: 0.0023 - val_loss: 1.4542e-04 - val_mae: 0.0020\n",
      "Epoch 16/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 1.8144e-04 - mae: 0.0024 - val_loss: 1.5484e-04 - val_mae: 0.0033\n",
      "Epoch 17/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - loss: 1.6264e-04 - mae: 0.0023 - val_loss: 1.1900e-04 - val_mae: 0.0027\n",
      "Epoch 18/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 1.5141e-04 - mae: 0.0023 - val_loss: 1.5573e-04 - val_mae: 0.0031\n",
      "Epoch 19/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - loss: 1.7637e-04 - mae: 0.0022 - val_loss: 1.0334e-04 - val_mae: 0.0012\n",
      "Epoch 20/20\n",
      "\u001b[1m1272/1272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 20ms/step - loss: 1.6842e-04 - mae: 0.0023 - val_loss: 1.0216e-04 - val_mae: 0.0015\n"
     ]
    }
   ],
   "source": [
    "from file_encoder import FileEncoder\n",
    "from data_manager import DataManager\n",
    "from audio_utils import N_MELS\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model\n",
    "\n",
    "\n",
    "TARGET_DIR = \"../thirdparty/「波音リツ」歌声データベースVer2/DATABASE\"\n",
    "OUTPUT_DIR = \"../master/ust/json\"\n",
    "    \n",
    "PITCH_FEATURES = 1 # MIDI番号を0 ~ 100のスカラー\n",
    "LYRIC_FEATURES = 1 # 歌詞をインデックスしたのでスカラー\n",
    "DURATION_FEATURES = 1 # msの長さを正規化したのでスカラー\n",
    "NOTE_CHUK_INDEX_FEATURES = 1 # 何番目の分割化を正規化したのでスカラー\n",
    "\n",
    "LYRIC_INDEX_MAX_DIM = 256 # インデックス化した歌詞の最大インデックス(237だったので若干多めに確保)\n",
    "LYRIC_DIM = 64 # インデックス化した歌詞を埋め込みした後の次元\n",
    "DURATION_DIM = 1 # 正規化した長さのミリ秒（0ミリ秒 ~ 10000ミリ秒（10秒）)\n",
    "\n",
    "# TODO: N分割したいが今後考える（ビブラートなどを考慮するにはノートを更に分割したい）\n",
    "NOTE_CHUNK = 1 # 1ノートに対して1分割する\n",
    "\n",
    "\n",
    "MODEL_FILE = \"../data/model.keras\"\n",
    "\n",
    "LYRIC_INDEX_FILE = \"../data/npy/lyric_indexs.npy\"\n",
    "DURATION_INDEX_FILE = \"../data/npy/duration_indexs.npy\"\n",
    "NOTENUM_INDEX_FILE = \"../data/npy/notenum_indexs.npy\"\n",
    "NAMES_FILE = \"../data/json/names.json\"\n",
    "Y_FILE = \"../data/npy/y.npy\"\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    pitch_input = layers.Input(shape=(NOTE_CHUNK,PITCH_FEATURES), name=\"pitch_input\")\n",
    "    lyric_input = layers.Input(shape=(NOTE_CHUNK,), name=\"lyric_input\")\n",
    "    duration_input = layers.Input(shape=(NOTE_CHUNK,DURATION_FEATURES), name=\"duration_input\")\n",
    "    # note_chunk_index_input = layers.Input(shape=(NOTE_CHUNK,NOTE_CHUK_INDEX_FEATURES), name=\"note_chunk_index_input\") # 何番目に分割したのか保持\n",
    "    \n",
    "    # 歌詞を埋め込み\n",
    "    lyric_embedding = layers.Embedding(output_dim=LYRIC_DIM, input_dim=LYRIC_INDEX_MAX_DIM, name=\"lyric_embedding\")(lyric_input) # 埋め込み\n",
    "    \n",
    "    # 時系列\n",
    "    lstm_pitch = layers.LSTM(units=64, return_sequences=True)(pitch_input)\n",
    "    lstm_duration = layers.LSTM(units=64, return_sequences=True)(duration_input)\n",
    "    lstm_lyric_embedding = layers.LSTM(units=64, return_sequences=True)(lyric_embedding)\n",
    "    # lstm_note_chunk_index = layers.LSTM(units=64, return_sequences=True)(note_chunk_index_input)\n",
    "    \n",
    "    \n",
    "    merged = layers.Concatenate(axis=-1, name=\"merged_features\")(\n",
    "        [lstm_pitch, lstm_lyric_embedding, lstm_duration]\n",
    "        # [lstm_pitch, lstm_lyric_embedding, lstm_duration, lstm_note_chunk_index]\n",
    "    )\n",
    "    \n",
    "    # 最終的な時系列処理\n",
    "    final_lstm = layers.LSTM(128, return_sequences=True, name=\"final_lstm\")(merged)\n",
    "    \n",
    "    # 出力層（例: メルスペクトログラムへの回帰）\n",
    "    output = layers.TimeDistributed(layers.Dense(5, activation=\"tanh\"), name=\"output\")(final_lstm)\n",
    "    \n",
    "    # モデル定義\n",
    "    model = Model(inputs=[pitch_input, lyric_input, duration_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    from_storage=False\n",
    "):\n",
    "    if from_storage:\n",
    "        lyric_indexs = np.load(LYRIC_INDEX_FILE)\n",
    "        durations = np.load(DURATION_INDEX_FILE)\n",
    "        notenums = np.load(NOTENUM_INDEX_FILE)\n",
    "        y = np.load(Y_FILE)\n",
    "        return lyric_indexs, durations, notenums, y\n",
    "        \n",
    "    encoder = FileEncoder(TARGET_DIR, OUTPUT_DIR)\n",
    "    lyric_indexs, durations, notenums, y = encoder.encode()\n",
    "    \n",
    "    lyric_indexs = np.array(lyric_indexs).reshape(-1, NOTE_CHUNK)  # 歌詞\n",
    "    durations = np.array(durations).reshape(-1, NOTE_CHUNK, DURATION_FEATURES)  # 長さ\n",
    "    notenums = np.array(notenums).reshape(-1, NOTE_CHUNK, PITCH_FEATURES)  # ピッチ\n",
    "    y = np.array(y)\n",
    "\n",
    "    np.save(LYRIC_INDEX_FILE, lyric_indexs)\n",
    "    np.save(DURATION_INDEX_FILE, durations)\n",
    "    np.save(NOTENUM_INDEX_FILE, notenums)\n",
    "    np.save(Y_FILE, y)\n",
    "\n",
    "    return lyric_indexs, durations, notenums, y\n",
    "\n",
    "# モデルの学習\n",
    "def train_model():\n",
    "    # モデル構築\n",
    "    model = build_model()\n",
    "\n",
    "    # モデルコンパイル\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    # データ読み込み\n",
    "    lyric_indexs, durations, notenums, y = load_data(from_storage=True)\n",
    "    print(\"lyric_indexs.shape\",  lyric_indexs.shape)\n",
    "    print(\"durations.shape\", durations.shape)\n",
    "    print(\"notenums.shahpe\", notenums.shape)\n",
    "    print(\"y.shape\", y.shape)\n",
    "\n",
    "    # モデルの学習\n",
    "    history = model.fit(\n",
    "        [notenums, lyric_indexs, durations], # 入力データ\n",
    "        y,  # ターゲットデータ\n",
    "        batch_size=32,\n",
    "        epochs=20,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    model.save(MODEL_FILE)\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_history(history):\n",
    "    # 損失を描画\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # 精度 (MAE) を描画\n",
    "    if 'mae' in history.history:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history.history['mae'], label='Training MAE')\n",
    "        if 'val_mae' in history.history:\n",
    "            plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "        plt.title('MAE over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "# 実行\n",
    "if __name__ == \"__main__\":\n",
    "    history = train_model()\n",
    "    plot_history(history)\n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38314443-0746-4ab0-a9ed-502d43d3bf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
